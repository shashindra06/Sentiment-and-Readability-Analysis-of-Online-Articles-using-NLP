Title: Building Custom TFLite Models and Benchmarking on VOXL2 Chips | Blackcoffer Insights

Our Success Stories

Banking Securities, and Insurance
Energy
Entertainment
Fast Moving Consumer Goods
Government & Think Tanks
Healthcare
Infrastructure & Real Estate
IT
Lifestyle & eCommerce
Production & manufacturing
Research & Academia
Retail & Supply Chain
Telecom


What We Do

Banking, Financials, Securities, and Insurance
Energy
Entertainment
Fast Moving Consumer Goods
Government & Think Tanks
Healthcare
Hospitality
Infrastructure & Real Estate
IT Services
Lifestyle, eCommerce & Online Market Place
News & Media
Production & Manufacturing
Research & Academia
Retail & Supply Chain


What We Think

Automobiles & Components
BFSI
Asset and Portfolio
Banks
Capital Markets
Derivatives and Securities
Diversified Financials
Finance & Accounting
Insurance
Securities and Capital Markets
Capital Goods
Commercial & Professional Services
Consumer Discretionary
Consumer Durables & Apparel
Consumer Services
Consumer Staples
Food & Staples Retailing
Food, Beverage & Tobacco
Household & Personal Products
Data Science
Analytics
Artificial Intelligence
Big Data
Business Analytics
Data Visualization
Internet of Things
Machine Learning
Statistics
Energy
DataOil


How To

Analytics
Application Development
Artificial Intelligence
Business Analytics
Example
Optimization
Projects
Software Development
Source Code Audit
Statistics
Web & Mobile App Development


Schedule Demo
Contact
 


FacebookLinkedinTwitterYoutube



 






Our Success Stories  

Transforming Real Estate Investments with an Automated Stack shares Platform


March 13, 2025 







Our Success Stories  

Empowering Careers: The Hirekingdom


March 13, 2025 







Our Success Stories  

Integrating Machine Learning Code into Kubeflow Pipeline – Kuberflow MLOps Kubernetes


October 24, 2024 







Our Success Stories  

Facial Recognition Attendance System


October 18, 2024 







What We Do  

AI audio and text conversational bot using livekit


November 30, 2024 







What We Do  

AI Receptionist | Voice Call Center | AI Lawyer | AI Sales Representative | AI Representative | AI Doctor | AI Coach | AI...


November 21, 2024 







What We Do  

Face Recognition with Deepfills Framework – Deepface


October 18, 2024 







What We Do  

Development of EA Robot for Automated Trading


September 15, 2024 







Utilities  

The Ultimate Collection of Multimedia Tools for Video Editing & Screen Recording (2024 Edition)


March 22, 2025 







What We Think  

Rising IT cities and its impact on the economy, environment, infrastructure, and city life by the year 2040.


August 24, 2023 







What We Think  

Rising IT Cities and Their Impact on the Economy, Environment, Infrastructure, and City Life in Future


August 18, 2023 







What We Think  

Internet Demand’s Evolution, Communication Impact, and 2035’s Alternative Pathways


August 18, 2023 







How To  

AI tools for mechanical engineering, categorized based on their applications


March 24, 2025 







How To  

Civil engineering AI Tools and Software


March 24, 2025 







How To  

AI tools and software for Electrical Engineering, categorized based on their applications


March 24, 2025 







How To  

Chemical engineering AI Tools & AI Software


March 24, 2025 






Home  Our Success Stories  Building Custom TFLite Models and Benchmarking on VOXL2 Chips





Our Success StoriesIT

Building Custom TFLite Models and Benchmarking on VOXL2 Chips

By Ajay Bidyarthy -   August 25, 2024  5188 





Client BackgroundClient: A leading tech consulting firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 100+The ProblemThe client aimed to explore the development and deployment of custom TensorFlow Lite (TFLite) models on VOXL2 hardware. The goal was to leverage the advanced GPU and NPU acceleration capabilities of VOXL2 to optimize and benchmark these models for efficient on-device inference. This project was not only about showcasing the potential of VOXL2 in enhancing machine learning performance but also about contributing to the broader understanding of deploying custom models on edge devices.Our SolutionLoad Base Model in ONNX Format: Started with loading a base model like YOLOv7 or YOLOv8 in ONNX format.
Convert ONNX Models to TFLite Format: Used the onnx-tf parser for conversion.
Quantize Models for VOXL2 Chips: Quantized models to float16 format for compatibility with VOXL2 chips.
Clone VOXL SDK Developer Environment: Cloned the VOXL SDK developer environment and set up ADB.
Connect VOXL2 Chip to Your Computer: Connected the VOXL2 chip and verified the connection.
Access VOXL Chip Shell: Accessed the shell of the VOXL chip for model deployment and configuration.
Create DEB Packages with Custom TFLite Models: Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model.
Use Custom DEB Package on VOXL2: Deployed the DEB package and configured the VOXL chip to run the model.
Run voxl-tflite-server: Executed voxl-tflite-server to start the inference process.
Verify Model Execution: Ensured the model runs without errors on the VOXL chip.Solution ArchitectureSteps were followed as referred in the Modal AI documentation. No solution architecture was required here.DeliverablesA Python script implementing the CVRP-TW model.
Test data and scripts for simulating different scenarios.
Documentation explaining how to use the model and interpret the results.Tech StackTools used
ONNX
TensorFlow Lite
VOXL SDK
Android Debug Bridge (ADB)
Language/techniques used
Python
Shell scripting
Models used
YOLOv7 or YOLOv8 in ONNX format
Mobilenet
Skills used
Machine Learning model conversion and optimization
Edge device deployment and configuration
Performance benchmarkingWhat are the technical Challenges Faced during Project ExecutionConverting ONNX models to TFLite format for compatibility with the TFLite runtime on VOXL chips.
Quantizing models to float16 format for compatibility with the GPU and DPU delegations on VOXL chips.
Setting up the VOXL SDK developer environment and ensuring ADB is correctly configured.
Deploying custom TFLite models on the VOXL chip and configuring it to run the models.
Benchmarking the model using the voxl-logger tool and encountering issues with the latest SDK build.How the Technical Challenges were SolvedUsed the onnx-tf parser for model conversion, ensuring compatibility.
Quantized models to float16 format, improving inference speed and reducing model size.
Cloned the VOXL SDK developer environment and followed the documentation to set up ADB.
Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model for deployment on the VOXL chip.
Consulted with the VOXL forums and developers for alternative methods of benchmarking due to SDK build issues.
Business Impact The successful deployment and benchmarking of custom TFLite models on VOXL2 chips have significantly enhanced the client’s ability to optimize machine learning performance on edge devices. By leveraging the advanced GPU and NPU acceleration capabilities of VOXL2, the client has been able to achieve efficient on-device inference, showcasing the potential of VOXL2 in the machine learning domain.Business ImpactThis project has not only contributed to the broader understanding of deploying custom models on edge devices but has also provided valuable insights into the performance of these models on VOXL chips. The process of overcoming technical challenges has further solidified the client’s confidence in the capabilities of VOXL2 and the potential of deploying custom TFLite models on edge devices.Overall, the Manu B VOXL project has been a success, demonstrating the potential of VOXL2 in enhancing machine learning performance on edge devices and contributing to the broader understanding of deploying custom models on edge devices. The project has highlighted the importance of overcoming technical challenges and the value of comprehensive guides for deploying and benchmarking TFLite models on VOXL chips.Project SnapshotsProject website urlForum :: https://forum.modalai.com/topic/3103/need-help-simulating-tflite-yolo-models-on-my-linux-machine/4?_=1711183909164 Report :: https://docs.google.com/document/d/17qVUzjCz3UKWB_-0fuBJzZAbdOULbp2a1U7b2Il3V78/edit SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy 

 

  
Previous articleSports Prediction Model for Multiple Sports LeaguesNext articleAnomaly Detection and Analysis for Enhanced Data Integrity and User Experience on Bright Data’s Website Ajay Bidyarthy  
 


 







 
 


Home  Our Success Stories  Building Custom TFLite Models and Benchmarking on VOXL2 Chips





Our Success StoriesIT

Building Custom TFLite Models and Benchmarking on VOXL2 Chips

By Ajay Bidyarthy -   August 25, 2024  5188 





Client BackgroundClient: A leading tech consulting firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 100+The ProblemThe client aimed to explore the development and deployment of custom TensorFlow Lite (TFLite) models on VOXL2 hardware. The goal was to leverage the advanced GPU and NPU acceleration capabilities of VOXL2 to optimize and benchmark these models for efficient on-device inference. This project was not only about showcasing the potential of VOXL2 in enhancing machine learning performance but also about contributing to the broader understanding of deploying custom models on edge devices.Our SolutionLoad Base Model in ONNX Format: Started with loading a base model like YOLOv7 or YOLOv8 in ONNX format.
Convert ONNX Models to TFLite Format: Used the onnx-tf parser for conversion.
Quantize Models for VOXL2 Chips: Quantized models to float16 format for compatibility with VOXL2 chips.
Clone VOXL SDK Developer Environment: Cloned the VOXL SDK developer environment and set up ADB.
Connect VOXL2 Chip to Your Computer: Connected the VOXL2 chip and verified the connection.
Access VOXL Chip Shell: Accessed the shell of the VOXL chip for model deployment and configuration.
Create DEB Packages with Custom TFLite Models: Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model.
Use Custom DEB Package on VOXL2: Deployed the DEB package and configured the VOXL chip to run the model.
Run voxl-tflite-server: Executed voxl-tflite-server to start the inference process.
Verify Model Execution: Ensured the model runs without errors on the VOXL chip.Solution ArchitectureSteps were followed as referred in the Modal AI documentation. No solution architecture was required here.DeliverablesA Python script implementing the CVRP-TW model.
Test data and scripts for simulating different scenarios.
Documentation explaining how to use the model and interpret the results.Tech StackTools used
ONNX
TensorFlow Lite
VOXL SDK
Android Debug Bridge (ADB)
Language/techniques used
Python
Shell scripting
Models used
YOLOv7 or YOLOv8 in ONNX format
Mobilenet
Skills used
Machine Learning model conversion and optimization
Edge device deployment and configuration
Performance benchmarkingWhat are the technical Challenges Faced during Project ExecutionConverting ONNX models to TFLite format for compatibility with the TFLite runtime on VOXL chips.
Quantizing models to float16 format for compatibility with the GPU and DPU delegations on VOXL chips.
Setting up the VOXL SDK developer environment and ensuring ADB is correctly configured.
Deploying custom TFLite models on the VOXL chip and configuring it to run the models.
Benchmarking the model using the voxl-logger tool and encountering issues with the latest SDK build.How the Technical Challenges were SolvedUsed the onnx-tf parser for model conversion, ensuring compatibility.
Quantized models to float16 format, improving inference speed and reducing model size.
Cloned the VOXL SDK developer environment and followed the documentation to set up ADB.
Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model for deployment on the VOXL chip.
Consulted with the VOXL forums and developers for alternative methods of benchmarking due to SDK build issues.
Business Impact The successful deployment and benchmarking of custom TFLite models on VOXL2 chips have significantly enhanced the client’s ability to optimize machine learning performance on edge devices. By leveraging the advanced GPU and NPU acceleration capabilities of VOXL2, the client has been able to achieve efficient on-device inference, showcasing the potential of VOXL2 in the machine learning domain.Business ImpactThis project has not only contributed to the broader understanding of deploying custom models on edge devices but has also provided valuable insights into the performance of these models on VOXL chips. The process of overcoming technical challenges has further solidified the client’s confidence in the capabilities of VOXL2 and the potential of deploying custom TFLite models on edge devices.Overall, the Manu B VOXL project has been a success, demonstrating the potential of VOXL2 in enhancing machine learning performance on edge devices and contributing to the broader understanding of deploying custom models on edge devices. The project has highlighted the importance of overcoming technical challenges and the value of comprehensive guides for deploying and benchmarking TFLite models on VOXL chips.Project SnapshotsProject website urlForum :: https://forum.modalai.com/topic/3103/need-help-simulating-tflite-yolo-models-on-my-linux-machine/4?_=1711183909164 Report :: https://docs.google.com/document/d/17qVUzjCz3UKWB_-0fuBJzZAbdOULbp2a1U7b2Il3V78/edit SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy 

 

  
Previous articleSports Prediction Model for Multiple Sports LeaguesNext articleAnomaly Detection and Analysis for Enhanced Data Integrity and User Experience on Bright Data’s Website Ajay Bidyarthy  
 


 







 





Our Success StoriesIT

Building Custom TFLite Models and Benchmarking on VOXL2 Chips

By Ajay Bidyarthy -   August 25, 2024  5188 





Client BackgroundClient: A leading tech consulting firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 100+The ProblemThe client aimed to explore the development and deployment of custom TensorFlow Lite (TFLite) models on VOXL2 hardware. The goal was to leverage the advanced GPU and NPU acceleration capabilities of VOXL2 to optimize and benchmark these models for efficient on-device inference. This project was not only about showcasing the potential of VOXL2 in enhancing machine learning performance but also about contributing to the broader understanding of deploying custom models on edge devices.Our SolutionLoad Base Model in ONNX Format: Started with loading a base model like YOLOv7 or YOLOv8 in ONNX format.
Convert ONNX Models to TFLite Format: Used the onnx-tf parser for conversion.
Quantize Models for VOXL2 Chips: Quantized models to float16 format for compatibility with VOXL2 chips.
Clone VOXL SDK Developer Environment: Cloned the VOXL SDK developer environment and set up ADB.
Connect VOXL2 Chip to Your Computer: Connected the VOXL2 chip and verified the connection.
Access VOXL Chip Shell: Accessed the shell of the VOXL chip for model deployment and configuration.
Create DEB Packages with Custom TFLite Models: Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model.
Use Custom DEB Package on VOXL2: Deployed the DEB package and configured the VOXL chip to run the model.
Run voxl-tflite-server: Executed voxl-tflite-server to start the inference process.
Verify Model Execution: Ensured the model runs without errors on the VOXL chip.Solution ArchitectureSteps were followed as referred in the Modal AI documentation. No solution architecture was required here.DeliverablesA Python script implementing the CVRP-TW model.
Test data and scripts for simulating different scenarios.
Documentation explaining how to use the model and interpret the results.Tech StackTools used
ONNX
TensorFlow Lite
VOXL SDK
Android Debug Bridge (ADB)
Language/techniques used
Python
Shell scripting
Models used
YOLOv7 or YOLOv8 in ONNX format
Mobilenet
Skills used
Machine Learning model conversion and optimization
Edge device deployment and configuration
Performance benchmarkingWhat are the technical Challenges Faced during Project ExecutionConverting ONNX models to TFLite format for compatibility with the TFLite runtime on VOXL chips.
Quantizing models to float16 format for compatibility with the GPU and DPU delegations on VOXL chips.
Setting up the VOXL SDK developer environment and ensuring ADB is correctly configured.
Deploying custom TFLite models on the VOXL chip and configuring it to run the models.
Benchmarking the model using the voxl-logger tool and encountering issues with the latest SDK build.How the Technical Challenges were SolvedUsed the onnx-tf parser for model conversion, ensuring compatibility.
Quantized models to float16 format, improving inference speed and reducing model size.
Cloned the VOXL SDK developer environment and followed the documentation to set up ADB.
Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model for deployment on the VOXL chip.
Consulted with the VOXL forums and developers for alternative methods of benchmarking due to SDK build issues.
Business Impact The successful deployment and benchmarking of custom TFLite models on VOXL2 chips have significantly enhanced the client’s ability to optimize machine learning performance on edge devices. By leveraging the advanced GPU and NPU acceleration capabilities of VOXL2, the client has been able to achieve efficient on-device inference, showcasing the potential of VOXL2 in the machine learning domain.Business ImpactThis project has not only contributed to the broader understanding of deploying custom models on edge devices but has also provided valuable insights into the performance of these models on VOXL chips. The process of overcoming technical challenges has further solidified the client’s confidence in the capabilities of VOXL2 and the potential of deploying custom TFLite models on edge devices.Overall, the Manu B VOXL project has been a success, demonstrating the potential of VOXL2 in enhancing machine learning performance on edge devices and contributing to the broader understanding of deploying custom models on edge devices. The project has highlighted the importance of overcoming technical challenges and the value of comprehensive guides for deploying and benchmarking TFLite models on VOXL chips.Project SnapshotsProject website urlForum :: https://forum.modalai.com/topic/3103/need-help-simulating-tflite-yolo-models-on-my-linux-machine/4?_=1711183909164 Report :: https://docs.google.com/document/d/17qVUzjCz3UKWB_-0fuBJzZAbdOULbp2a1U7b2Il3V78/edit SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy 

 

  
Previous articleSports Prediction Model for Multiple Sports LeaguesNext articleAnomaly Detection and Analysis for Enhanced Data Integrity and User Experience on Bright Data’s Website Ajay Bidyarthy  
 


 





Our Success StoriesIT

Building Custom TFLite Models and Benchmarking on VOXL2 Chips

By Ajay Bidyarthy -   August 25, 2024  5188 





Client BackgroundClient: A leading tech consulting firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 100+The ProblemThe client aimed to explore the development and deployment of custom TensorFlow Lite (TFLite) models on VOXL2 hardware. The goal was to leverage the advanced GPU and NPU acceleration capabilities of VOXL2 to optimize and benchmark these models for efficient on-device inference. This project was not only about showcasing the potential of VOXL2 in enhancing machine learning performance but also about contributing to the broader understanding of deploying custom models on edge devices.Our SolutionLoad Base Model in ONNX Format: Started with loading a base model like YOLOv7 or YOLOv8 in ONNX format.
Convert ONNX Models to TFLite Format: Used the onnx-tf parser for conversion.
Quantize Models for VOXL2 Chips: Quantized models to float16 format for compatibility with VOXL2 chips.
Clone VOXL SDK Developer Environment: Cloned the VOXL SDK developer environment and set up ADB.
Connect VOXL2 Chip to Your Computer: Connected the VOXL2 chip and verified the connection.
Access VOXL Chip Shell: Accessed the shell of the VOXL chip for model deployment and configuration.
Create DEB Packages with Custom TFLite Models: Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model.
Use Custom DEB Package on VOXL2: Deployed the DEB package and configured the VOXL chip to run the model.
Run voxl-tflite-server: Executed voxl-tflite-server to start the inference process.
Verify Model Execution: Ensured the model runs without errors on the VOXL chip.Solution ArchitectureSteps were followed as referred in the Modal AI documentation. No solution architecture was required here.DeliverablesA Python script implementing the CVRP-TW model.
Test data and scripts for simulating different scenarios.
Documentation explaining how to use the model and interpret the results.Tech StackTools used
ONNX
TensorFlow Lite
VOXL SDK
Android Debug Bridge (ADB)
Language/techniques used
Python
Shell scripting
Models used
YOLOv7 or YOLOv8 in ONNX format
Mobilenet
Skills used
Machine Learning model conversion and optimization
Edge device deployment and configuration
Performance benchmarkingWhat are the technical Challenges Faced during Project ExecutionConverting ONNX models to TFLite format for compatibility with the TFLite runtime on VOXL chips.
Quantizing models to float16 format for compatibility with the GPU and DPU delegations on VOXL chips.
Setting up the VOXL SDK developer environment and ensuring ADB is correctly configured.
Deploying custom TFLite models on the VOXL chip and configuring it to run the models.
Benchmarking the model using the voxl-logger tool and encountering issues with the latest SDK build.How the Technical Challenges were SolvedUsed the onnx-tf parser for model conversion, ensuring compatibility.
Quantized models to float16 format, improving inference speed and reducing model size.
Cloned the VOXL SDK developer environment and followed the documentation to set up ADB.
Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model for deployment on the VOXL chip.
Consulted with the VOXL forums and developers for alternative methods of benchmarking due to SDK build issues.
Business Impact The successful deployment and benchmarking of custom TFLite models on VOXL2 chips have significantly enhanced the client’s ability to optimize machine learning performance on edge devices. By leveraging the advanced GPU and NPU acceleration capabilities of VOXL2, the client has been able to achieve efficient on-device inference, showcasing the potential of VOXL2 in the machine learning domain.Business ImpactThis project has not only contributed to the broader understanding of deploying custom models on edge devices but has also provided valuable insights into the performance of these models on VOXL chips. The process of overcoming technical challenges has further solidified the client’s confidence in the capabilities of VOXL2 and the potential of deploying custom TFLite models on edge devices.Overall, the Manu B VOXL project has been a success, demonstrating the potential of VOXL2 in enhancing machine learning performance on edge devices and contributing to the broader understanding of deploying custom models on edge devices. The project has highlighted the importance of overcoming technical challenges and the value of comprehensive guides for deploying and benchmarking TFLite models on VOXL chips.Project SnapshotsProject website urlForum :: https://forum.modalai.com/topic/3103/need-help-simulating-tflite-yolo-models-on-my-linux-machine/4?_=1711183909164 Report :: https://docs.google.com/document/d/17qVUzjCz3UKWB_-0fuBJzZAbdOULbp2a1U7b2Il3V78/edit SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy 

 

  
Previous articleSports Prediction Model for Multiple Sports LeaguesNext articleAnomaly Detection and Analysis for Enhanced Data Integrity and User Experience on Bright Data’s Website Ajay Bidyarthy  
 


 



Our Success StoriesIT

Building Custom TFLite Models and Benchmarking on VOXL2 Chips

By Ajay Bidyarthy -   August 25, 2024  5188 





Client BackgroundClient: A leading tech consulting firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 100+The ProblemThe client aimed to explore the development and deployment of custom TensorFlow Lite (TFLite) models on VOXL2 hardware. The goal was to leverage the advanced GPU and NPU acceleration capabilities of VOXL2 to optimize and benchmark these models for efficient on-device inference. This project was not only about showcasing the potential of VOXL2 in enhancing machine learning performance but also about contributing to the broader understanding of deploying custom models on edge devices.Our SolutionLoad Base Model in ONNX Format: Started with loading a base model like YOLOv7 or YOLOv8 in ONNX format.
Convert ONNX Models to TFLite Format: Used the onnx-tf parser for conversion.
Quantize Models for VOXL2 Chips: Quantized models to float16 format for compatibility with VOXL2 chips.
Clone VOXL SDK Developer Environment: Cloned the VOXL SDK developer environment and set up ADB.
Connect VOXL2 Chip to Your Computer: Connected the VOXL2 chip and verified the connection.
Access VOXL Chip Shell: Accessed the shell of the VOXL chip for model deployment and configuration.
Create DEB Packages with Custom TFLite Models: Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model.
Use Custom DEB Package on VOXL2: Deployed the DEB package and configured the VOXL chip to run the model.
Run voxl-tflite-server: Executed voxl-tflite-server to start the inference process.
Verify Model Execution: Ensured the model runs without errors on the VOXL chip.Solution ArchitectureSteps were followed as referred in the Modal AI documentation. No solution architecture was required here.DeliverablesA Python script implementing the CVRP-TW model.
Test data and scripts for simulating different scenarios.
Documentation explaining how to use the model and interpret the results.Tech StackTools used
ONNX
TensorFlow Lite
VOXL SDK
Android Debug Bridge (ADB)
Language/techniques used
Python
Shell scripting
Models used
YOLOv7 or YOLOv8 in ONNX format
Mobilenet
Skills used
Machine Learning model conversion and optimization
Edge device deployment and configuration
Performance benchmarkingWhat are the technical Challenges Faced during Project ExecutionConverting ONNX models to TFLite format for compatibility with the TFLite runtime on VOXL chips.
Quantizing models to float16 format for compatibility with the GPU and DPU delegations on VOXL chips.
Setting up the VOXL SDK developer environment and ensuring ADB is correctly configured.
Deploying custom TFLite models on the VOXL chip and configuring it to run the models.
Benchmarking the model using the voxl-logger tool and encountering issues with the latest SDK build.How the Technical Challenges were SolvedUsed the onnx-tf parser for model conversion, ensuring compatibility.
Quantized models to float16 format, improving inference speed and reducing model size.
Cloned the VOXL SDK developer environment and followed the documentation to set up ADB.
Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model for deployment on the VOXL chip.
Consulted with the VOXL forums and developers for alternative methods of benchmarking due to SDK build issues.
Business Impact The successful deployment and benchmarking of custom TFLite models on VOXL2 chips have significantly enhanced the client’s ability to optimize machine learning performance on edge devices. By leveraging the advanced GPU and NPU acceleration capabilities of VOXL2, the client has been able to achieve efficient on-device inference, showcasing the potential of VOXL2 in the machine learning domain.Business ImpactThis project has not only contributed to the broader understanding of deploying custom models on edge devices but has also provided valuable insights into the performance of these models on VOXL chips. The process of overcoming technical challenges has further solidified the client’s confidence in the capabilities of VOXL2 and the potential of deploying custom TFLite models on edge devices.Overall, the Manu B VOXL project has been a success, demonstrating the potential of VOXL2 in enhancing machine learning performance on edge devices and contributing to the broader understanding of deploying custom models on edge devices. The project has highlighted the importance of overcoming technical challenges and the value of comprehensive guides for deploying and benchmarking TFLite models on VOXL chips.Project SnapshotsProject website urlForum :: https://forum.modalai.com/topic/3103/need-help-simulating-tflite-yolo-models-on-my-linux-machine/4?_=1711183909164 Report :: https://docs.google.com/document/d/17qVUzjCz3UKWB_-0fuBJzZAbdOULbp2a1U7b2Il3V78/edit SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy 

 

  
Previous articleSports Prediction Model for Multiple Sports LeaguesNext articleAnomaly Detection and Analysis for Enhanced Data Integrity and User Experience on Bright Data’s Website Ajay Bidyarthy  


Our Success StoriesIT

Building Custom TFLite Models and Benchmarking on VOXL2 Chips

By Ajay Bidyarthy -   August 25, 2024  5188 


By Ajay Bidyarthy -  
5188



Client BackgroundClient: A leading tech consulting firm in the USAIndustry Type: ITProducts & Services: IT Consulting, IT Support, SaaSOrganization Size: 100+The ProblemThe client aimed to explore the development and deployment of custom TensorFlow Lite (TFLite) models on VOXL2 hardware. The goal was to leverage the advanced GPU and NPU acceleration capabilities of VOXL2 to optimize and benchmark these models for efficient on-device inference. This project was not only about showcasing the potential of VOXL2 in enhancing machine learning performance but also about contributing to the broader understanding of deploying custom models on edge devices.Our SolutionLoad Base Model in ONNX Format: Started with loading a base model like YOLOv7 or YOLOv8 in ONNX format.
Convert ONNX Models to TFLite Format: Used the onnx-tf parser for conversion.
Quantize Models for VOXL2 Chips: Quantized models to float16 format for compatibility with VOXL2 chips.
Clone VOXL SDK Developer Environment: Cloned the VOXL SDK developer environment and set up ADB.
Connect VOXL2 Chip to Your Computer: Connected the VOXL2 chip and verified the connection.
Access VOXL Chip Shell: Accessed the shell of the VOXL chip for model deployment and configuration.
Create DEB Packages with Custom TFLite Models: Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model.
Use Custom DEB Package on VOXL2: Deployed the DEB package and configured the VOXL chip to run the model.
Run voxl-tflite-server: Executed voxl-tflite-server to start the inference process.
Verify Model Execution: Ensured the model runs without errors on the VOXL chip.Solution ArchitectureSteps were followed as referred in the Modal AI documentation. No solution architecture was required here.DeliverablesA Python script implementing the CVRP-TW model.
Test data and scripts for simulating different scenarios.
Documentation explaining how to use the model and interpret the results.Tech StackTools used
ONNX
TensorFlow Lite
VOXL SDK
Android Debug Bridge (ADB)
Language/techniques used
Python
Shell scripting
Models used
YOLOv7 or YOLOv8 in ONNX format
Mobilenet
Skills used
Machine Learning model conversion and optimization
Edge device deployment and configuration
Performance benchmarkingWhat are the technical Challenges Faced during Project ExecutionConverting ONNX models to TFLite format for compatibility with the TFLite runtime on VOXL chips.
Quantizing models to float16 format for compatibility with the GPU and DPU delegations on VOXL chips.
Setting up the VOXL SDK developer environment and ensuring ADB is correctly configured.
Deploying custom TFLite models on the VOXL chip and configuring it to run the models.
Benchmarking the model using the voxl-logger tool and encountering issues with the latest SDK build.How the Technical Challenges were SolvedUsed the onnx-tf parser for model conversion, ensuring compatibility.
Quantized models to float16 format, improving inference speed and reducing model size.
Cloned the VOXL SDK developer environment and followed the documentation to set up ADB.
Cloned the voxl-tflite-server repository, copied TFLite files, and configured the model for deployment on the VOXL chip.
Consulted with the VOXL forums and developers for alternative methods of benchmarking due to SDK build issues.
Business Impact The successful deployment and benchmarking of custom TFLite models on VOXL2 chips have significantly enhanced the client’s ability to optimize machine learning performance on edge devices. By leveraging the advanced GPU and NPU acceleration capabilities of VOXL2, the client has been able to achieve efficient on-device inference, showcasing the potential of VOXL2 in the machine learning domain.Business ImpactThis project has not only contributed to the broader understanding of deploying custom models on edge devices but has also provided valuable insights into the performance of these models on VOXL chips. The process of overcoming technical challenges has further solidified the client’s confidence in the capabilities of VOXL2 and the potential of deploying custom TFLite models on edge devices.Overall, the Manu B VOXL project has been a success, demonstrating the potential of VOXL2 in enhancing machine learning performance on edge devices and contributing to the broader understanding of deploying custom models on edge devices. The project has highlighted the importance of overcoming technical challenges and the value of comprehensive guides for deploying and benchmarking TFLite models on VOXL chips.Project SnapshotsProject website urlForum :: https://forum.modalai.com/topic/3103/need-help-simulating-tflite-yolo-models-on-my-linux-machine/4?_=1711183909164 Report :: https://docs.google.com/document/d/17qVUzjCz3UKWB_-0fuBJzZAbdOULbp2a1U7b2Il3V78/edit SummarizeSummarized: https://blackcoffer.com/This project was done by the Blackcoffer Team, a Global IT Consulting firm.Contact DetailsThis solution was designed and developed by Blackcoffer TeamHere are my contact details:Firm Name: Blackcoffer Pvt. Ltd.Firm Website: www.blackcoffer.comFirm Address: 4/2, E-Extension, Shaym Vihar Phase 1, New Delhi 110043Email: ajay@blackcoffer.comSkype: asbidyarthyWhatsApp: +91 9717367468Telegram: @asbidyarthy 


  
Previous articleSports Prediction Model for Multiple Sports LeaguesNext articleAnomaly Detection and Analysis for Enhanced Data Integrity and User Experience on Bright Data’s Website
Previous articleSports Prediction Model for Multiple Sports Leagues
Previous articleSports Prediction Model for Multiple Sports Leagues
Next articleAnomaly Detection and Analysis for Enhanced Data Integrity and User Experience on Bright Data’s Website
Next articleAnomaly Detection and Analysis for Enhanced Data Integrity and User Experience on Bright Data’s Website



 

Review: Penalty Shoot Out de Evoplay en Casinos Online para México


June 7, 2025 







 

Disparo al Gol: Todo sobre el “Penalty Shoot Out” de Evoplay y los Casinos con Retiro Inmediato en México


June 7, 2025 







 

Penal Shoot Out de Evoplay: Un Juego que Captura la Emoción del Fútbol en Casinos Mexicanos


June 7, 2025 







 

Review: Penalty Shoot Out de Evoplay en Casinos Online para México


June 7, 2025 







 

Disparo al Gol: Todo sobre el “Penalty Shoot Out” de Evoplay y los Casinos con Retiro Inmediato en México


June 7, 2025 







 

Penal Shoot Out de Evoplay: Un Juego que Captura la Emoción del Fútbol en Casinos Mexicanos


June 7, 2025 







ABOUT US


FOLLOW US


FacebookLinkedinTwitterYoutube