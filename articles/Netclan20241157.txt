Title: Stocktwits Data Structurization | Blackcoffer Insights

Our Success Stories

Banking Securities, and Insurance
Energy
Entertainment
Fast Moving Consumer Goods
Government & Think Tanks
Healthcare
Infrastructure & Real Estate
IT
Lifestyle & eCommerce
Production & manufacturing
Research & Academia
Retail & Supply Chain
Telecom


What We Do

Banking, Financials, Securities, and Insurance
Energy
Entertainment
Fast Moving Consumer Goods
Government & Think Tanks
Healthcare
Hospitality
Infrastructure & Real Estate
IT Services
Lifestyle, eCommerce & Online Market Place
News & Media
Production & Manufacturing
Research & Academia
Retail & Supply Chain


What We Think

Automobiles & Components
BFSI
Asset and Portfolio
Banks
Capital Markets
Derivatives and Securities
Diversified Financials
Finance & Accounting
Insurance
Securities and Capital Markets
Capital Goods
Commercial & Professional Services
Consumer Discretionary
Consumer Durables & Apparel
Consumer Services
Consumer Staples
Food & Staples Retailing
Food, Beverage & Tobacco
Household & Personal Products
Data Science
Analytics
Artificial Intelligence
Big Data
Business Analytics
Data Visualization
Internet of Things
Machine Learning
Statistics
Energy
DataOil


How To

Analytics
Application Development
Artificial Intelligence
Business Analytics
Example
Optimization
Projects
Software Development
Source Code Audit
Statistics
Web & Mobile App Development


Schedule Demo
Contact
 


FacebookLinkedinTwitterYoutube



 






Our Success Stories  

Transforming Real Estate Investments with an Automated Stack shares Platform


March 13, 2025 







Our Success Stories  

Empowering Careers: The Hirekingdom


March 13, 2025 







Our Success Stories  

Integrating Machine Learning Code into Kubeflow Pipeline – Kuberflow MLOps Kubernetes


October 24, 2024 







Our Success Stories  

Facial Recognition Attendance System


October 18, 2024 







What We Do  

AI audio and text conversational bot using livekit


November 30, 2024 







What We Do  

AI Receptionist | Voice Call Center | AI Lawyer | AI Sales Representative | AI Representative | AI Doctor | AI Coach | AI...


November 21, 2024 







What We Do  

Face Recognition with Deepfills Framework – Deepface


October 18, 2024 







What We Do  

Development of EA Robot for Automated Trading


September 15, 2024 







Utilities  

The Ultimate Collection of Multimedia Tools for Video Editing & Screen Recording (2024 Edition)


March 22, 2025 







What We Think  

Rising IT cities and its impact on the economy, environment, infrastructure, and city life by the year 2040.


August 24, 2023 







What We Think  

Rising IT Cities and Their Impact on the Economy, Environment, Infrastructure, and City Life in Future


August 18, 2023 







What We Think  

Internet Demand’s Evolution, Communication Impact, and 2035’s Alternative Pathways


August 18, 2023 







How To  

AI tools for mechanical engineering, categorized based on their applications


March 24, 2025 







How To  

Civil engineering AI Tools and Software


March 24, 2025 







How To  

AI tools and software for Electrical Engineering, categorized based on their applications


March 24, 2025 







How To  

Chemical engineering AI Tools & AI Software


March 24, 2025 






Home  Our Success Stories  Stocktwits Data Structurization





Our Success StoriesBanking Securities, and Insurance

Stocktwits Data Structurization

By Ajay Bidyarthy -   August 30, 2021  9086 





Client BackgroundClient: A leading financial institution in the USAIndustry Type: Financial services & ConsultingServices: Financial consultantOrganization Size: 100+Project Objective>To process two json file stocktwits_legacy_msg_2015_10.txt (file size = 2 GB) & stocktwits_legacy_msg_2015_10.txt (file size = 3.5 GB).>To handle Nested Json for both files and after conversion into one merged Data Frame need to perform Data Structurization.>While accessing a Json file in JupyterNB, I need to perform Chunking as the file size is bigger and it is in json format to avoid PC standstill.>After Data Preprocessing I need to perform Exploratory Data Analysis on that Data.> Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DescriptionDuring the training period I was involved with 2 live projects, One project named ‘Stocktwits Data Structurization’ in which I have to process huge JSON Data which was already obtained the size of data was nearly 5 GB need to process the data by chunking with chunk size = 20000 rows at a time. The file has nested JSON data within it’s attributes so abstracts data from the nested columns into a new dataframe. Completed handling complex nested json formed columns abstracted from nested json. Then need to Handle the missing data by mapping it with another index dataset further missing values for certain attributes were handled by mean value and 0 substitution. This task involves numerous pandas operations along with multiple python functions. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes.Our SolutionWorked on Accessing Json Data, done tree Analysis on Json Sample data.Both the File was too big for reading and applying some Python Code in JupyterNb, so performed chunking of stocktwits_legacy_messages_2015_10.txt  with chunk size = 20000 rows at a time. Similarly trying for the other file.Created a list of all the chunked files of Json Data & Concat all the files in that list.The File has Nested Json data within it’s attributes so abstracted data from the nested columns into a new DataFrame. Completed handling complex nested json formed columns abstracted from nested json.Renamed the columns with identification. (Eg: ‘id’ as ‘entities_id’) likewise for others. So that while merging the data doesn’t create any issue. Completed forming Preprocessed csv file for 1st json file which  Output2015.csv.For Second file size was > 3gb so splitted the file into ten parts and then individually solved nested json for all these parts like done in the 1st file finally concat them into one, then handled columns arrangements and removed unwanted columns and finally removed dictionary representation from entity_sentiments column. Completed forming Preprocessed csv file for 2nd json file which is Output_Stocktwits_2017.csv.The cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DeliverablesCategorized Preprocessed CSV FilesPython ScriptiPython NB with comments on each performed code.Tools used● Jupyter Notebook● Anaconda● Notepad++● Sublime Text● Brackets● JsonViewerLanguage/techniques used● Python ProgrammingModels usedMy project ‘Stocktwits Data Structurization’ developed with a software model which makes the project high quality, reliable and cost effective.● Software Model : RAD(Rapid Application Development model) Model● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.● Advantages of RAD Model:o Changing requirements can be accommodated.o Progress can be measured.o Iteration time can be short with use of powerful RAD tools.o Productivity with fewer people in a short time.o Reduced development time.o Increases reusability of components.o Quick initial reviews occur.o Encourages customer feedback.o Integration from very beginning solves a lot of integration issuesSkills used● Data Mining● Data Wrangling● Data Visualization● Python Programming including OOPs and Exception HandlingDatabases usedNo Databases were used, all the data was stored on Google Drive and Local Device.Web Cloud Servers usedNo Cloud Server were usedWhat are the technical Challenges Faced during Project Execution● Handling Huge Data and Data Cleaning● JSON Data Serialization.● Solving Complex Nested JSON among the data provided.How the Technical Challenges were Solved● Handling Huge Data and Data CleaningSolved by Breaking the Dataset into 10 stream parts as the data was too huge and was not able to read easily in Jupyter NB.● JSON Data SerializationSolved by Data Chunking with chunk_size=20000 which means serialization of data with processing 20000 rows at a time.● Solving Complex Nested JSON among the data provided.Viewed the Structure of the part of data in JSON Viewer then Changed the data in proper standard JSON Format. After Reading JSON Data Performing Normalization of Nested JSON data setting maximum level of normalization with specifying proper orient form. Then After Normalization remaining Unsolved Nested JSON was solved using Dictionary Conversions and Structuring the data. Project Snapshots Figure 1 Sample Input Dataframe After Converting Outer JSONFigure 2 Sample Output Dataframe After Solving Nested JSON and Data Preprocessing 

 

  
Previous articleHow artificial intelligence can boost your productivity level?Next articleMarbles Stimulation using python Ajay Bidyarthy  
 


 







 
 


Home  Our Success Stories  Stocktwits Data Structurization





Our Success StoriesBanking Securities, and Insurance

Stocktwits Data Structurization

By Ajay Bidyarthy -   August 30, 2021  9086 





Client BackgroundClient: A leading financial institution in the USAIndustry Type: Financial services & ConsultingServices: Financial consultantOrganization Size: 100+Project Objective>To process two json file stocktwits_legacy_msg_2015_10.txt (file size = 2 GB) & stocktwits_legacy_msg_2015_10.txt (file size = 3.5 GB).>To handle Nested Json for both files and after conversion into one merged Data Frame need to perform Data Structurization.>While accessing a Json file in JupyterNB, I need to perform Chunking as the file size is bigger and it is in json format to avoid PC standstill.>After Data Preprocessing I need to perform Exploratory Data Analysis on that Data.> Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DescriptionDuring the training period I was involved with 2 live projects, One project named ‘Stocktwits Data Structurization’ in which I have to process huge JSON Data which was already obtained the size of data was nearly 5 GB need to process the data by chunking with chunk size = 20000 rows at a time. The file has nested JSON data within it’s attributes so abstracts data from the nested columns into a new dataframe. Completed handling complex nested json formed columns abstracted from nested json. Then need to Handle the missing data by mapping it with another index dataset further missing values for certain attributes were handled by mean value and 0 substitution. This task involves numerous pandas operations along with multiple python functions. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes.Our SolutionWorked on Accessing Json Data, done tree Analysis on Json Sample data.Both the File was too big for reading and applying some Python Code in JupyterNb, so performed chunking of stocktwits_legacy_messages_2015_10.txt  with chunk size = 20000 rows at a time. Similarly trying for the other file.Created a list of all the chunked files of Json Data & Concat all the files in that list.The File has Nested Json data within it’s attributes so abstracted data from the nested columns into a new DataFrame. Completed handling complex nested json formed columns abstracted from nested json.Renamed the columns with identification. (Eg: ‘id’ as ‘entities_id’) likewise for others. So that while merging the data doesn’t create any issue. Completed forming Preprocessed csv file for 1st json file which  Output2015.csv.For Second file size was > 3gb so splitted the file into ten parts and then individually solved nested json for all these parts like done in the 1st file finally concat them into one, then handled columns arrangements and removed unwanted columns and finally removed dictionary representation from entity_sentiments column. Completed forming Preprocessed csv file for 2nd json file which is Output_Stocktwits_2017.csv.The cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DeliverablesCategorized Preprocessed CSV FilesPython ScriptiPython NB with comments on each performed code.Tools used● Jupyter Notebook● Anaconda● Notepad++● Sublime Text● Brackets● JsonViewerLanguage/techniques used● Python ProgrammingModels usedMy project ‘Stocktwits Data Structurization’ developed with a software model which makes the project high quality, reliable and cost effective.● Software Model : RAD(Rapid Application Development model) Model● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.● Advantages of RAD Model:o Changing requirements can be accommodated.o Progress can be measured.o Iteration time can be short with use of powerful RAD tools.o Productivity with fewer people in a short time.o Reduced development time.o Increases reusability of components.o Quick initial reviews occur.o Encourages customer feedback.o Integration from very beginning solves a lot of integration issuesSkills used● Data Mining● Data Wrangling● Data Visualization● Python Programming including OOPs and Exception HandlingDatabases usedNo Databases were used, all the data was stored on Google Drive and Local Device.Web Cloud Servers usedNo Cloud Server were usedWhat are the technical Challenges Faced during Project Execution● Handling Huge Data and Data Cleaning● JSON Data Serialization.● Solving Complex Nested JSON among the data provided.How the Technical Challenges were Solved● Handling Huge Data and Data CleaningSolved by Breaking the Dataset into 10 stream parts as the data was too huge and was not able to read easily in Jupyter NB.● JSON Data SerializationSolved by Data Chunking with chunk_size=20000 which means serialization of data with processing 20000 rows at a time.● Solving Complex Nested JSON among the data provided.Viewed the Structure of the part of data in JSON Viewer then Changed the data in proper standard JSON Format. After Reading JSON Data Performing Normalization of Nested JSON data setting maximum level of normalization with specifying proper orient form. Then After Normalization remaining Unsolved Nested JSON was solved using Dictionary Conversions and Structuring the data. Project Snapshots Figure 1 Sample Input Dataframe After Converting Outer JSONFigure 2 Sample Output Dataframe After Solving Nested JSON and Data Preprocessing 

 

  
Previous articleHow artificial intelligence can boost your productivity level?Next articleMarbles Stimulation using python Ajay Bidyarthy  
 


 







 





Our Success StoriesBanking Securities, and Insurance

Stocktwits Data Structurization

By Ajay Bidyarthy -   August 30, 2021  9086 





Client BackgroundClient: A leading financial institution in the USAIndustry Type: Financial services & ConsultingServices: Financial consultantOrganization Size: 100+Project Objective>To process two json file stocktwits_legacy_msg_2015_10.txt (file size = 2 GB) & stocktwits_legacy_msg_2015_10.txt (file size = 3.5 GB).>To handle Nested Json for both files and after conversion into one merged Data Frame need to perform Data Structurization.>While accessing a Json file in JupyterNB, I need to perform Chunking as the file size is bigger and it is in json format to avoid PC standstill.>After Data Preprocessing I need to perform Exploratory Data Analysis on that Data.> Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DescriptionDuring the training period I was involved with 2 live projects, One project named ‘Stocktwits Data Structurization’ in which I have to process huge JSON Data which was already obtained the size of data was nearly 5 GB need to process the data by chunking with chunk size = 20000 rows at a time. The file has nested JSON data within it’s attributes so abstracts data from the nested columns into a new dataframe. Completed handling complex nested json formed columns abstracted from nested json. Then need to Handle the missing data by mapping it with another index dataset further missing values for certain attributes were handled by mean value and 0 substitution. This task involves numerous pandas operations along with multiple python functions. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes.Our SolutionWorked on Accessing Json Data, done tree Analysis on Json Sample data.Both the File was too big for reading and applying some Python Code in JupyterNb, so performed chunking of stocktwits_legacy_messages_2015_10.txt  with chunk size = 20000 rows at a time. Similarly trying for the other file.Created a list of all the chunked files of Json Data & Concat all the files in that list.The File has Nested Json data within it’s attributes so abstracted data from the nested columns into a new DataFrame. Completed handling complex nested json formed columns abstracted from nested json.Renamed the columns with identification. (Eg: ‘id’ as ‘entities_id’) likewise for others. So that while merging the data doesn’t create any issue. Completed forming Preprocessed csv file for 1st json file which  Output2015.csv.For Second file size was > 3gb so splitted the file into ten parts and then individually solved nested json for all these parts like done in the 1st file finally concat them into one, then handled columns arrangements and removed unwanted columns and finally removed dictionary representation from entity_sentiments column. Completed forming Preprocessed csv file for 2nd json file which is Output_Stocktwits_2017.csv.The cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DeliverablesCategorized Preprocessed CSV FilesPython ScriptiPython NB with comments on each performed code.Tools used● Jupyter Notebook● Anaconda● Notepad++● Sublime Text● Brackets● JsonViewerLanguage/techniques used● Python ProgrammingModels usedMy project ‘Stocktwits Data Structurization’ developed with a software model which makes the project high quality, reliable and cost effective.● Software Model : RAD(Rapid Application Development model) Model● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.● Advantages of RAD Model:o Changing requirements can be accommodated.o Progress can be measured.o Iteration time can be short with use of powerful RAD tools.o Productivity with fewer people in a short time.o Reduced development time.o Increases reusability of components.o Quick initial reviews occur.o Encourages customer feedback.o Integration from very beginning solves a lot of integration issuesSkills used● Data Mining● Data Wrangling● Data Visualization● Python Programming including OOPs and Exception HandlingDatabases usedNo Databases were used, all the data was stored on Google Drive and Local Device.Web Cloud Servers usedNo Cloud Server were usedWhat are the technical Challenges Faced during Project Execution● Handling Huge Data and Data Cleaning● JSON Data Serialization.● Solving Complex Nested JSON among the data provided.How the Technical Challenges were Solved● Handling Huge Data and Data CleaningSolved by Breaking the Dataset into 10 stream parts as the data was too huge and was not able to read easily in Jupyter NB.● JSON Data SerializationSolved by Data Chunking with chunk_size=20000 which means serialization of data with processing 20000 rows at a time.● Solving Complex Nested JSON among the data provided.Viewed the Structure of the part of data in JSON Viewer then Changed the data in proper standard JSON Format. After Reading JSON Data Performing Normalization of Nested JSON data setting maximum level of normalization with specifying proper orient form. Then After Normalization remaining Unsolved Nested JSON was solved using Dictionary Conversions and Structuring the data. Project Snapshots Figure 1 Sample Input Dataframe After Converting Outer JSONFigure 2 Sample Output Dataframe After Solving Nested JSON and Data Preprocessing 

 

  
Previous articleHow artificial intelligence can boost your productivity level?Next articleMarbles Stimulation using python Ajay Bidyarthy  
 


 





Our Success StoriesBanking Securities, and Insurance

Stocktwits Data Structurization

By Ajay Bidyarthy -   August 30, 2021  9086 





Client BackgroundClient: A leading financial institution in the USAIndustry Type: Financial services & ConsultingServices: Financial consultantOrganization Size: 100+Project Objective>To process two json file stocktwits_legacy_msg_2015_10.txt (file size = 2 GB) & stocktwits_legacy_msg_2015_10.txt (file size = 3.5 GB).>To handle Nested Json for both files and after conversion into one merged Data Frame need to perform Data Structurization.>While accessing a Json file in JupyterNB, I need to perform Chunking as the file size is bigger and it is in json format to avoid PC standstill.>After Data Preprocessing I need to perform Exploratory Data Analysis on that Data.> Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DescriptionDuring the training period I was involved with 2 live projects, One project named ‘Stocktwits Data Structurization’ in which I have to process huge JSON Data which was already obtained the size of data was nearly 5 GB need to process the data by chunking with chunk size = 20000 rows at a time. The file has nested JSON data within it’s attributes so abstracts data from the nested columns into a new dataframe. Completed handling complex nested json formed columns abstracted from nested json. Then need to Handle the missing data by mapping it with another index dataset further missing values for certain attributes were handled by mean value and 0 substitution. This task involves numerous pandas operations along with multiple python functions. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes.Our SolutionWorked on Accessing Json Data, done tree Analysis on Json Sample data.Both the File was too big for reading and applying some Python Code in JupyterNb, so performed chunking of stocktwits_legacy_messages_2015_10.txt  with chunk size = 20000 rows at a time. Similarly trying for the other file.Created a list of all the chunked files of Json Data & Concat all the files in that list.The File has Nested Json data within it’s attributes so abstracted data from the nested columns into a new DataFrame. Completed handling complex nested json formed columns abstracted from nested json.Renamed the columns with identification. (Eg: ‘id’ as ‘entities_id’) likewise for others. So that while merging the data doesn’t create any issue. Completed forming Preprocessed csv file for 1st json file which  Output2015.csv.For Second file size was > 3gb so splitted the file into ten parts and then individually solved nested json for all these parts like done in the 1st file finally concat them into one, then handled columns arrangements and removed unwanted columns and finally removed dictionary representation from entity_sentiments column. Completed forming Preprocessed csv file for 2nd json file which is Output_Stocktwits_2017.csv.The cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DeliverablesCategorized Preprocessed CSV FilesPython ScriptiPython NB with comments on each performed code.Tools used● Jupyter Notebook● Anaconda● Notepad++● Sublime Text● Brackets● JsonViewerLanguage/techniques used● Python ProgrammingModels usedMy project ‘Stocktwits Data Structurization’ developed with a software model which makes the project high quality, reliable and cost effective.● Software Model : RAD(Rapid Application Development model) Model● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.● Advantages of RAD Model:o Changing requirements can be accommodated.o Progress can be measured.o Iteration time can be short with use of powerful RAD tools.o Productivity with fewer people in a short time.o Reduced development time.o Increases reusability of components.o Quick initial reviews occur.o Encourages customer feedback.o Integration from very beginning solves a lot of integration issuesSkills used● Data Mining● Data Wrangling● Data Visualization● Python Programming including OOPs and Exception HandlingDatabases usedNo Databases were used, all the data was stored on Google Drive and Local Device.Web Cloud Servers usedNo Cloud Server were usedWhat are the technical Challenges Faced during Project Execution● Handling Huge Data and Data Cleaning● JSON Data Serialization.● Solving Complex Nested JSON among the data provided.How the Technical Challenges were Solved● Handling Huge Data and Data CleaningSolved by Breaking the Dataset into 10 stream parts as the data was too huge and was not able to read easily in Jupyter NB.● JSON Data SerializationSolved by Data Chunking with chunk_size=20000 which means serialization of data with processing 20000 rows at a time.● Solving Complex Nested JSON among the data provided.Viewed the Structure of the part of data in JSON Viewer then Changed the data in proper standard JSON Format. After Reading JSON Data Performing Normalization of Nested JSON data setting maximum level of normalization with specifying proper orient form. Then After Normalization remaining Unsolved Nested JSON was solved using Dictionary Conversions and Structuring the data. Project Snapshots Figure 1 Sample Input Dataframe After Converting Outer JSONFigure 2 Sample Output Dataframe After Solving Nested JSON and Data Preprocessing 

 

  
Previous articleHow artificial intelligence can boost your productivity level?Next articleMarbles Stimulation using python Ajay Bidyarthy  
 


 



Our Success StoriesBanking Securities, and Insurance

Stocktwits Data Structurization

By Ajay Bidyarthy -   August 30, 2021  9086 





Client BackgroundClient: A leading financial institution in the USAIndustry Type: Financial services & ConsultingServices: Financial consultantOrganization Size: 100+Project Objective>To process two json file stocktwits_legacy_msg_2015_10.txt (file size = 2 GB) & stocktwits_legacy_msg_2015_10.txt (file size = 3.5 GB).>To handle Nested Json for both files and after conversion into one merged Data Frame need to perform Data Structurization.>While accessing a Json file in JupyterNB, I need to perform Chunking as the file size is bigger and it is in json format to avoid PC standstill.>After Data Preprocessing I need to perform Exploratory Data Analysis on that Data.> Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DescriptionDuring the training period I was involved with 2 live projects, One project named ‘Stocktwits Data Structurization’ in which I have to process huge JSON Data which was already obtained the size of data was nearly 5 GB need to process the data by chunking with chunk size = 20000 rows at a time. The file has nested JSON data within it’s attributes so abstracts data from the nested columns into a new dataframe. Completed handling complex nested json formed columns abstracted from nested json. Then need to Handle the missing data by mapping it with another index dataset further missing values for certain attributes were handled by mean value and 0 substitution. This task involves numerous pandas operations along with multiple python functions. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes.Our SolutionWorked on Accessing Json Data, done tree Analysis on Json Sample data.Both the File was too big for reading and applying some Python Code in JupyterNb, so performed chunking of stocktwits_legacy_messages_2015_10.txt  with chunk size = 20000 rows at a time. Similarly trying for the other file.Created a list of all the chunked files of Json Data & Concat all the files in that list.The File has Nested Json data within it’s attributes so abstracted data from the nested columns into a new DataFrame. Completed handling complex nested json formed columns abstracted from nested json.Renamed the columns with identification. (Eg: ‘id’ as ‘entities_id’) likewise for others. So that while merging the data doesn’t create any issue. Completed forming Preprocessed csv file for 1st json file which  Output2015.csv.For Second file size was > 3gb so splitted the file into ten parts and then individually solved nested json for all these parts like done in the 1st file finally concat them into one, then handled columns arrangements and removed unwanted columns and finally removed dictionary representation from entity_sentiments column. Completed forming Preprocessed csv file for 2nd json file which is Output_Stocktwits_2017.csv.The cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DeliverablesCategorized Preprocessed CSV FilesPython ScriptiPython NB with comments on each performed code.Tools used● Jupyter Notebook● Anaconda● Notepad++● Sublime Text● Brackets● JsonViewerLanguage/techniques used● Python ProgrammingModels usedMy project ‘Stocktwits Data Structurization’ developed with a software model which makes the project high quality, reliable and cost effective.● Software Model : RAD(Rapid Application Development model) Model● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.● Advantages of RAD Model:o Changing requirements can be accommodated.o Progress can be measured.o Iteration time can be short with use of powerful RAD tools.o Productivity with fewer people in a short time.o Reduced development time.o Increases reusability of components.o Quick initial reviews occur.o Encourages customer feedback.o Integration from very beginning solves a lot of integration issuesSkills used● Data Mining● Data Wrangling● Data Visualization● Python Programming including OOPs and Exception HandlingDatabases usedNo Databases were used, all the data was stored on Google Drive and Local Device.Web Cloud Servers usedNo Cloud Server were usedWhat are the technical Challenges Faced during Project Execution● Handling Huge Data and Data Cleaning● JSON Data Serialization.● Solving Complex Nested JSON among the data provided.How the Technical Challenges were Solved● Handling Huge Data and Data CleaningSolved by Breaking the Dataset into 10 stream parts as the data was too huge and was not able to read easily in Jupyter NB.● JSON Data SerializationSolved by Data Chunking with chunk_size=20000 which means serialization of data with processing 20000 rows at a time.● Solving Complex Nested JSON among the data provided.Viewed the Structure of the part of data in JSON Viewer then Changed the data in proper standard JSON Format. After Reading JSON Data Performing Normalization of Nested JSON data setting maximum level of normalization with specifying proper orient form. Then After Normalization remaining Unsolved Nested JSON was solved using Dictionary Conversions and Structuring the data. Project Snapshots Figure 1 Sample Input Dataframe After Converting Outer JSONFigure 2 Sample Output Dataframe After Solving Nested JSON and Data Preprocessing 

 

  
Previous articleHow artificial intelligence can boost your productivity level?Next articleMarbles Stimulation using python Ajay Bidyarthy  


Our Success StoriesBanking Securities, and Insurance

Stocktwits Data Structurization

By Ajay Bidyarthy -   August 30, 2021  9086 


By Ajay Bidyarthy -  
9086



Client BackgroundClient: A leading financial institution in the USAIndustry Type: Financial services & ConsultingServices: Financial consultantOrganization Size: 100+Project Objective>To process two json file stocktwits_legacy_msg_2015_10.txt (file size = 2 GB) & stocktwits_legacy_msg_2015_10.txt (file size = 3.5 GB).>To handle Nested Json for both files and after conversion into one merged Data Frame need to perform Data Structurization.>While accessing a Json file in JupyterNB, I need to perform Chunking as the file size is bigger and it is in json format to avoid PC standstill.>After Data Preprocessing I need to perform Exploratory Data Analysis on that Data.> Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DescriptionDuring the training period I was involved with 2 live projects, One project named ‘Stocktwits Data Structurization’ in which I have to process huge JSON Data which was already obtained the size of data was nearly 5 GB need to process the data by chunking with chunk size = 20000 rows at a time. The file has nested JSON data within it’s attributes so abstracts data from the nested columns into a new dataframe. Completed handling complex nested json formed columns abstracted from nested json. Then need to Handle the missing data by mapping it with another index dataset further missing values for certain attributes were handled by mean value and 0 substitution. This task involves numerous pandas operations along with multiple python functions. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes.Our SolutionWorked on Accessing Json Data, done tree Analysis on Json Sample data.Both the File was too big for reading and applying some Python Code in JupyterNb, so performed chunking of stocktwits_legacy_messages_2015_10.txt  with chunk size = 20000 rows at a time. Similarly trying for the other file.Created a list of all the chunked files of Json Data & Concat all the files in that list.The File has Nested Json data within it’s attributes so abstracted data from the nested columns into a new DataFrame. Completed handling complex nested json formed columns abstracted from nested json.Renamed the columns with identification. (Eg: ‘id’ as ‘entities_id’) likewise for others. So that while merging the data doesn’t create any issue. Completed forming Preprocessed csv file for 1st json file which  Output2015.csv.For Second file size was > 3gb so splitted the file into ten parts and then individually solved nested json for all these parts like done in the 1st file finally concat them into one, then handled columns arrangements and removed unwanted columns and finally removed dictionary representation from entity_sentiments column. Completed forming Preprocessed csv file for 2nd json file which is Output_Stocktwits_2017.csv.The cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Conditional Programming to deal with Data Transferring to a particular folder based on the column values.Project DeliverablesCategorized Preprocessed CSV FilesPython ScriptiPython NB with comments on each performed code.Tools used● Jupyter Notebook● Anaconda● Notepad++● Sublime Text● Brackets● JsonViewerLanguage/techniques used● Python ProgrammingModels usedMy project ‘Stocktwits Data Structurization’ developed with a software model which makes the project high quality, reliable and cost effective.● Software Model : RAD(Rapid Application Development model) Model● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.● Advantages of RAD Model:o Changing requirements can be accommodated.o Progress can be measured.o Iteration time can be short with use of powerful RAD tools.o Productivity with fewer people in a short time.o Reduced development time.o Increases reusability of components.o Quick initial reviews occur.o Encourages customer feedback.o Integration from very beginning solves a lot of integration issuesSkills used● Data Mining● Data Wrangling● Data Visualization● Python Programming including OOPs and Exception HandlingDatabases usedNo Databases were used, all the data was stored on Google Drive and Local Device.Web Cloud Servers usedNo Cloud Server were usedWhat are the technical Challenges Faced during Project Execution● Handling Huge Data and Data Cleaning● JSON Data Serialization.● Solving Complex Nested JSON among the data provided.How the Technical Challenges were Solved● Handling Huge Data and Data CleaningSolved by Breaking the Dataset into 10 stream parts as the data was too huge and was not able to read easily in Jupyter NB.● JSON Data SerializationSolved by Data Chunking with chunk_size=20000 which means serialization of data with processing 20000 rows at a time.● Solving Complex Nested JSON among the data provided.Viewed the Structure of the part of data in JSON Viewer then Changed the data in proper standard JSON Format. After Reading JSON Data Performing Normalization of Nested JSON data setting maximum level of normalization with specifying proper orient form. Then After Normalization remaining Unsolved Nested JSON was solved using Dictionary Conversions and Structuring the data. Project Snapshots Figure 1 Sample Input Dataframe After Converting Outer JSONFigure 2 Sample Output Dataframe After Solving Nested JSON and Data Preprocessing 


  
Previous articleHow artificial intelligence can boost your productivity level?Next articleMarbles Stimulation using python
Previous articleHow artificial intelligence can boost your productivity level?
Previous articleHow artificial intelligence can boost your productivity level?
Next articleMarbles Stimulation using python
Next articleMarbles Stimulation using python



 

Review: Penalty Shoot Out de Evoplay en Casinos Online para México


June 7, 2025 







 

Disparo al Gol: Todo sobre el “Penalty Shoot Out” de Evoplay y los Casinos con Retiro Inmediato en México


June 7, 2025 







 

Penal Shoot Out de Evoplay: Un Juego que Captura la Emoción del Fútbol en Casinos Mexicanos


June 7, 2025 







 

Review: Penalty Shoot Out de Evoplay en Casinos Online para México


June 7, 2025 







 

Disparo al Gol: Todo sobre el “Penalty Shoot Out” de Evoplay y los Casinos con Retiro Inmediato en México


June 7, 2025 







 

Penal Shoot Out de Evoplay: Un Juego que Captura la Emoción del Fútbol en Casinos Mexicanos


June 7, 2025 







ABOUT US


FOLLOW US


FacebookLinkedinTwitterYoutube