Title: Data integration and big data performance using Elasticsearch | Blackcoffer Insights

Our Success Stories

Banking Securities, and Insurance
Energy
Entertainment
Fast Moving Consumer Goods
Government & Think Tanks
Healthcare
Infrastructure & Real Estate
IT
Lifestyle & eCommerce
Production & manufacturing
Research & Academia
Retail & Supply Chain
Telecom


What We Do

Banking, Financials, Securities, and Insurance
Energy
Entertainment
Fast Moving Consumer Goods
Government & Think Tanks
Healthcare
Hospitality
Infrastructure & Real Estate
IT Services
Lifestyle, eCommerce & Online Market Place
News & Media
Production & Manufacturing
Research & Academia
Retail & Supply Chain


What We Think

Automobiles & Components
BFSI
Asset and Portfolio
Banks
Capital Markets
Derivatives and Securities
Diversified Financials
Finance & Accounting
Insurance
Securities and Capital Markets
Capital Goods
Commercial & Professional Services
Consumer Discretionary
Consumer Durables & Apparel
Consumer Services
Consumer Staples
Food & Staples Retailing
Food, Beverage & Tobacco
Household & Personal Products
Data Science
Analytics
Artificial Intelligence
Big Data
Business Analytics
Data Visualization
Internet of Things
Machine Learning
Statistics
Energy
DataOil


How To

Analytics
Application Development
Artificial Intelligence
Business Analytics
Example
Optimization
Projects
Software Development
Source Code Audit
Statistics
Web & Mobile App Development


Schedule Demo
Contact
 


FacebookLinkedinTwitterYoutube



 






Our Success Stories  

Transforming Real Estate Investments with an Automated Stack shares Platform


March 13, 2025 







Our Success Stories  

Empowering Careers: The Hirekingdom


March 13, 2025 







Our Success Stories  

Integrating Machine Learning Code into Kubeflow Pipeline – Kuberflow MLOps Kubernetes


October 24, 2024 







Our Success Stories  

Facial Recognition Attendance System


October 18, 2024 







What We Do  

AI audio and text conversational bot using livekit


November 30, 2024 







What We Do  

AI Receptionist | Voice Call Center | AI Lawyer | AI Sales Representative | AI Representative | AI Doctor | AI Coach | AI...


November 21, 2024 







What We Do  

Face Recognition with Deepfills Framework – Deepface


October 18, 2024 







What We Do  

Development of EA Robot for Automated Trading


September 15, 2024 







Utilities  

The Ultimate Collection of Multimedia Tools for Video Editing & Screen Recording (2024 Edition)


March 22, 2025 







What We Think  

Rising IT cities and its impact on the economy, environment, infrastructure, and city life by the year 2040.


August 24, 2023 







What We Think  

Rising IT Cities and Their Impact on the Economy, Environment, Infrastructure, and City Life in Future


August 18, 2023 







What We Think  

Internet Demand’s Evolution, Communication Impact, and 2035’s Alternative Pathways


August 18, 2023 







How To  

AI tools for mechanical engineering, categorized based on their applications


March 24, 2025 







How To  

Civil engineering AI Tools and Software


March 24, 2025 







How To  

AI tools and software for Electrical Engineering, categorized based on their applications


March 24, 2025 







How To  

Chemical engineering AI Tools & AI Software


March 24, 2025 






Home  Our Success Stories  Data integration and big data performance using Elasticsearch





Our Success StoriesIT

Data integration and big data performance using Elasticsearch

By Ajay Bidyarthy -   July 13, 2022  9098 





Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT & ConsultingServices: Software, Business Solutions, ConsultingOrganization Size: 200+Project ObjectiveMigrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.Project DescriptionThe client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.Our SolutionSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Write a pipeline file (.conf file) which is used to ingest data from postgres to elasticsearch. The datatypes of cloumns, unique constraints, datetime formats etc., are all defined in this file. This is executed with the help of logstash. Once the data is inserted, it can be queried in the kibana’s built in query compiler. Here we can check the veracity of the data.Identify the code in the backend that needs to be changed.Replace this code with new code that would now query elasticserach. We use elastic_query_builder module for this.Testing Postgres and Elasticsearch performance.Project DeliverablesSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Pipeline i.e; logstash fileNew working backend code for elasticsearchCommands to check elastic data.Customizable logstash pipelineTools usedElasticsearchPostmanKibanaLogstashPythonJavascriptAmazon Web ServicesPostgresDockerGit BucketGithubLanguage/techniques usedJavascriptJsonDomain-Specific Language for elasticsearchbashSkills usedElasticsearch query knowledgePostgres query knowledgeNetworkingJavascriptBackend web stackDatabases usedPostgresElasticsearchWeb Cloud Servers usedAmazon Web Services (AWS)What are the technical Challenges Faced during Project ExecutionSometimes for large responses from elasticsearch ( size above 500mb), time taken was above 30 secs.How the Technical Challenges were SolvedTo solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.Business ImpactEarlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.Project Snapshots  

 

  
Previous articleWeb Data ConnectorNext articleAuvik, Connectwise integration in Grafana Ajay Bidyarthy  
 


 







 
 


Home  Our Success Stories  Data integration and big data performance using Elasticsearch





Our Success StoriesIT

Data integration and big data performance using Elasticsearch

By Ajay Bidyarthy -   July 13, 2022  9098 





Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT & ConsultingServices: Software, Business Solutions, ConsultingOrganization Size: 200+Project ObjectiveMigrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.Project DescriptionThe client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.Our SolutionSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Write a pipeline file (.conf file) which is used to ingest data from postgres to elasticsearch. The datatypes of cloumns, unique constraints, datetime formats etc., are all defined in this file. This is executed with the help of logstash. Once the data is inserted, it can be queried in the kibana’s built in query compiler. Here we can check the veracity of the data.Identify the code in the backend that needs to be changed.Replace this code with new code that would now query elasticserach. We use elastic_query_builder module for this.Testing Postgres and Elasticsearch performance.Project DeliverablesSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Pipeline i.e; logstash fileNew working backend code for elasticsearchCommands to check elastic data.Customizable logstash pipelineTools usedElasticsearchPostmanKibanaLogstashPythonJavascriptAmazon Web ServicesPostgresDockerGit BucketGithubLanguage/techniques usedJavascriptJsonDomain-Specific Language for elasticsearchbashSkills usedElasticsearch query knowledgePostgres query knowledgeNetworkingJavascriptBackend web stackDatabases usedPostgresElasticsearchWeb Cloud Servers usedAmazon Web Services (AWS)What are the technical Challenges Faced during Project ExecutionSometimes for large responses from elasticsearch ( size above 500mb), time taken was above 30 secs.How the Technical Challenges were SolvedTo solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.Business ImpactEarlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.Project Snapshots  

 

  
Previous articleWeb Data ConnectorNext articleAuvik, Connectwise integration in Grafana Ajay Bidyarthy  
 


 







 





Our Success StoriesIT

Data integration and big data performance using Elasticsearch

By Ajay Bidyarthy -   July 13, 2022  9098 





Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT & ConsultingServices: Software, Business Solutions, ConsultingOrganization Size: 200+Project ObjectiveMigrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.Project DescriptionThe client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.Our SolutionSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Write a pipeline file (.conf file) which is used to ingest data from postgres to elasticsearch. The datatypes of cloumns, unique constraints, datetime formats etc., are all defined in this file. This is executed with the help of logstash. Once the data is inserted, it can be queried in the kibana’s built in query compiler. Here we can check the veracity of the data.Identify the code in the backend that needs to be changed.Replace this code with new code that would now query elasticserach. We use elastic_query_builder module for this.Testing Postgres and Elasticsearch performance.Project DeliverablesSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Pipeline i.e; logstash fileNew working backend code for elasticsearchCommands to check elastic data.Customizable logstash pipelineTools usedElasticsearchPostmanKibanaLogstashPythonJavascriptAmazon Web ServicesPostgresDockerGit BucketGithubLanguage/techniques usedJavascriptJsonDomain-Specific Language for elasticsearchbashSkills usedElasticsearch query knowledgePostgres query knowledgeNetworkingJavascriptBackend web stackDatabases usedPostgresElasticsearchWeb Cloud Servers usedAmazon Web Services (AWS)What are the technical Challenges Faced during Project ExecutionSometimes for large responses from elasticsearch ( size above 500mb), time taken was above 30 secs.How the Technical Challenges were SolvedTo solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.Business ImpactEarlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.Project Snapshots  

 

  
Previous articleWeb Data ConnectorNext articleAuvik, Connectwise integration in Grafana Ajay Bidyarthy  
 


 





Our Success StoriesIT

Data integration and big data performance using Elasticsearch

By Ajay Bidyarthy -   July 13, 2022  9098 





Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT & ConsultingServices: Software, Business Solutions, ConsultingOrganization Size: 200+Project ObjectiveMigrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.Project DescriptionThe client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.Our SolutionSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Write a pipeline file (.conf file) which is used to ingest data from postgres to elasticsearch. The datatypes of cloumns, unique constraints, datetime formats etc., are all defined in this file. This is executed with the help of logstash. Once the data is inserted, it can be queried in the kibana’s built in query compiler. Here we can check the veracity of the data.Identify the code in the backend that needs to be changed.Replace this code with new code that would now query elasticserach. We use elastic_query_builder module for this.Testing Postgres and Elasticsearch performance.Project DeliverablesSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Pipeline i.e; logstash fileNew working backend code for elasticsearchCommands to check elastic data.Customizable logstash pipelineTools usedElasticsearchPostmanKibanaLogstashPythonJavascriptAmazon Web ServicesPostgresDockerGit BucketGithubLanguage/techniques usedJavascriptJsonDomain-Specific Language for elasticsearchbashSkills usedElasticsearch query knowledgePostgres query knowledgeNetworkingJavascriptBackend web stackDatabases usedPostgresElasticsearchWeb Cloud Servers usedAmazon Web Services (AWS)What are the technical Challenges Faced during Project ExecutionSometimes for large responses from elasticsearch ( size above 500mb), time taken was above 30 secs.How the Technical Challenges were SolvedTo solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.Business ImpactEarlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.Project Snapshots  

 

  
Previous articleWeb Data ConnectorNext articleAuvik, Connectwise integration in Grafana Ajay Bidyarthy  
 


 



Our Success StoriesIT

Data integration and big data performance using Elasticsearch

By Ajay Bidyarthy -   July 13, 2022  9098 





Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT & ConsultingServices: Software, Business Solutions, ConsultingOrganization Size: 200+Project ObjectiveMigrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.Project DescriptionThe client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.Our SolutionSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Write a pipeline file (.conf file) which is used to ingest data from postgres to elasticsearch. The datatypes of cloumns, unique constraints, datetime formats etc., are all defined in this file. This is executed with the help of logstash. Once the data is inserted, it can be queried in the kibana’s built in query compiler. Here we can check the veracity of the data.Identify the code in the backend that needs to be changed.Replace this code with new code that would now query elasticserach. We use elastic_query_builder module for this.Testing Postgres and Elasticsearch performance.Project DeliverablesSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Pipeline i.e; logstash fileNew working backend code for elasticsearchCommands to check elastic data.Customizable logstash pipelineTools usedElasticsearchPostmanKibanaLogstashPythonJavascriptAmazon Web ServicesPostgresDockerGit BucketGithubLanguage/techniques usedJavascriptJsonDomain-Specific Language for elasticsearchbashSkills usedElasticsearch query knowledgePostgres query knowledgeNetworkingJavascriptBackend web stackDatabases usedPostgresElasticsearchWeb Cloud Servers usedAmazon Web Services (AWS)What are the technical Challenges Faced during Project ExecutionSometimes for large responses from elasticsearch ( size above 500mb), time taken was above 30 secs.How the Technical Challenges were SolvedTo solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.Business ImpactEarlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.Project Snapshots  

 

  
Previous articleWeb Data ConnectorNext articleAuvik, Connectwise integration in Grafana Ajay Bidyarthy  


Our Success StoriesIT

Data integration and big data performance using Elasticsearch

By Ajay Bidyarthy -   July 13, 2022  9098 


By Ajay Bidyarthy -  
9098



Client BackgroundClient: A Leading Tech Firm in the USAIndustry Type: IT & ConsultingServices: Software, Business Solutions, ConsultingOrganization Size: 200+Project ObjectiveMigrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.Project DescriptionThe client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.Our SolutionSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Write a pipeline file (.conf file) which is used to ingest data from postgres to elasticsearch. The datatypes of cloumns, unique constraints, datetime formats etc., are all defined in this file. This is executed with the help of logstash. Once the data is inserted, it can be queried in the kibana’s built in query compiler. Here we can check the veracity of the data.Identify the code in the backend that needs to be changed.Replace this code with new code that would now query elasticserach. We use elastic_query_builder module for this.Testing Postgres and Elasticsearch performance.Project DeliverablesSetup ELK stack (Elasticsearch, Logstash, Kibana) on AWS EC2 instance.Pipeline i.e; logstash fileNew working backend code for elasticsearchCommands to check elastic data.Customizable logstash pipelineTools usedElasticsearchPostmanKibanaLogstashPythonJavascriptAmazon Web ServicesPostgresDockerGit BucketGithubLanguage/techniques usedJavascriptJsonDomain-Specific Language for elasticsearchbashSkills usedElasticsearch query knowledgePostgres query knowledgeNetworkingJavascriptBackend web stackDatabases usedPostgresElasticsearchWeb Cloud Servers usedAmazon Web Services (AWS)What are the technical Challenges Faced during Project ExecutionSometimes for large responses from elasticsearch ( size above 500mb), time taken was above 30 secs.How the Technical Challenges were SolvedTo solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.Business ImpactEarlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.Project Snapshots  


  
Previous articleWeb Data ConnectorNext articleAuvik, Connectwise integration in Grafana
Previous articleWeb Data Connector
Previous articleWeb Data Connector
Next articleAuvik, Connectwise integration in Grafana
Next articleAuvik, Connectwise integration in Grafana



 

Review: Penalty Shoot Out de Evoplay en Casinos Online para México


June 7, 2025 







 

Disparo al Gol: Todo sobre el “Penalty Shoot Out” de Evoplay y los Casinos con Retiro Inmediato en México


June 7, 2025 







 

Penal Shoot Out de Evoplay: Un Juego que Captura la Emoción del Fútbol en Casinos Mexicanos


June 7, 2025 







 

Review: Penalty Shoot Out de Evoplay en Casinos Online para México


June 7, 2025 







 

Disparo al Gol: Todo sobre el “Penalty Shoot Out” de Evoplay y los Casinos con Retiro Inmediato en México


June 7, 2025 







 

Penal Shoot Out de Evoplay: Un Juego que Captura la Emoción del Fútbol en Casinos Mexicanos


June 7, 2025 







ABOUT US


FOLLOW US


FacebookLinkedinTwitterYoutube